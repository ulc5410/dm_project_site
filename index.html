---
layout: default
title: Home
---

<h1 class="post-title">Introduction</h1>
<p>Judges from many states use risk assessment scores as their reference during criminal sentencing. It is important for those risk assessment scores to be unbiased.</p>
<p>Our project idea is to examine biases in records, mitigate those biases and propose a fair recidivism prediction model. Then leverage the interpretation model LIME to show the decision of the NN model.
</p>

<h1 class="post-title">Problem Background</h1>
<ul>
  <li>It is increasingly common in courtrooms across the nation that use risk assessments. They are used to inform decisions about who can be set free at every stage of the criminal justice system, from assigning bond amounts. In Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin, the results of such assessments are given to judges during criminal sentencing.</li>
  <li>Northpointe (now Equivant) is a company in justice industry that provide the tools and expertise to reduce risks. COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is their algorithm designed for judging if criminal defendants would recidivism in the future. Broward County primarily uses the score to determine whether to release or detain a defendant before his or her trial in the hope of addressing jail overcrowding. <a href='https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html#document/p8/a296604'>Here is a sample survey Northpointe used to calculate COMPAS scores</a></li>
  <li>ProPublica obtained two years’ worth of COMPAS scores from the Broward County Sheriff’s Office in Florida through a public records request. The data contains 11,757 people who were assessed at the pretrial stage.</li>
  <li>ProPublica’s analysis shows that there is racial bias in COMPAS predictions. However, the algorithm predicting recidivism is biased in favor of white defendants, and against black defendants.</li>
  <li>As fairness is getting more attention recent years, researchers develope metrics to quantify fairness in decisions and mitigate any bias and unfairness issues in ML. Figure 1 shows the number of papers, starting in 2010, that have been published in the fairness in ML domain. The figure shows that number of papers sharply increased starting in 2016 and 2017. Below is the figure 1 from <a href='https://arxiv.org/pdf/2010.04053.pdf'> FAIRNESS IN MACHINE LEARNING: A SURVEY</a>.</li>
</ul>
<img src="../public/img/fig1.png" class="img-nice">

<h1 class="post-title">Motivation</h1>
<ul>
<li>There are always injustice cases in the world. If an unfair model trained by biased datasets plays a role in verdicts, there will be more injustice cases.
</li>
<li>The project goal is to mine the root causes of the biased dataset by digging into it with data mining techniques.
</li>
<li>Based on the insights from the analysis, we will try to mitigate those biases and propose a fair recidivism prediction model.
</li>
<li>Moreover, the deep model’s interpretability is bad → leverage the interpretation model LIME to show the decision of the NN model.
</li>
</ul>